<!DOCTYPE html>
<html>
<head>

    <!-- Custom styles for this template -->
    <link href="files/jumbotron.css" rel="stylesheet">

    <script src="js/main.js"></script>
    <script src="js/scroll.js"></script>
    <title>Yijie Zhang</title>

    <!-- Meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- <link rel="icon" href="./static/images/shail_logo.jpeg"> -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
        
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/shail_logo.jpeg">

    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

    <meta name='viewport' content='width=device-width, initial-scale=1'>
    <!-- <script src='https://kit.fontawesome.com/a076d05399.js' crossorigin='anonymous'></script> -->

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha512-KfkfwYDsLkIlwQp6LFnl8zNdLGxu9YAA1QvwINks4PhcElQSvqcyVLLD9aMhXd13uQjoXtEKNosOWaZqXgel0g==" crossorigin="anonymous" referrerpolicy="no-referrer" />

    <!-- Custom Styles -->
    <style>
          body {
            font-family: 'sans-serif';
            font-size: 16px;
            background-color: #FFFFFF;
            /* color: #4F6071; */
            color: #383d42;
          }
          #header {
            background-color: #f4f4f4;
            /*background-color: #FFFFFF;*/
            display: flex;
            align-items: flex-end;
            padding-top:40px;
            padding-bottom:60px;
          }
          #footer {
            background-color: #FFFFFF;
            padding:60px;
          }
          #portrait {
            border: 3px solid white;
          }
          #header-text {
            margin-top: 60px;
            margin-left: 220px;
          }
          #header-text-name {
            font-size: 40px;
          }
          #header-text-email {
            font-size: 20px;
            /* font-style: italic; */
          }
          .header-text-desc {
            font-size: 20px;
          }
          .vspace-top {
            margin-top: 30px;
          }
          .vspace-top-news {
              margin-top: 15px;
          }
          .paper-image {
            width: 150px;
          }
          .news-date {
              font-weight: bold;
          }
          .paper-title {
            font-weight: bold;
          }
          .paper-authors {
            /* font-style: italic; */
            font-size: 14px;
          }
          .paper-desc {
            font-size: 16px;
            font-style: oblique;
          }
          
          /* a {
            color: rgb(178, 51, 51);
            background-color: transparent;
            text-decoration: none;
          } */

          .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 60%;
          }
          
          .expandable-section .content {
                display: none;
                padding: 10px;
                border: 1px solid #ccc;
                margin-top: 5px;
            }

          .material-icons {
              vertical-align: -6px;
          }

          /* .toggle-button {
              background-color: #f4f4f4;
              border: 1px solid #ccc;
              padding: 5px;
              cursor: pointer;
          } */
          .toggle-button {
              cursor: pointer;
          }

          .highlight-bg {
            background-color: #f4f4f4; /* Light blue background */
            padding: 0px;
            border-radius: 5px;
            display: inline-block;
            margin-bottom: 0; /* Add some space below */
          }

          .research-section {
            font-size: 10;
            color: #0f1410;
            margin-bottom: 0;
          }

    </style>
</head>

<body>
    <div id='header'>
    <div class='container'>
        <div class='row'>
            <div class="col-sm-3 offset-sm-1">
                <img src='imgs/portrait.JPG' class='img-fluid' id='portrait' style="max-width: 70%; max-height: 30;">
            </div>

            <div class="col">
                <div id='header-text-name'>
                    Yijie Zhang
                </div>
                <div id='header-text-email'>
                    yijiezhang(at)ucla(dot)edu
                </div>
                <div>
                    <a href="https://github.com/Yijie-Zhang">
                        <span class="icon">
                            <i class="fab fa-github" style='font-size:20px'></i>
                        </span> |
                    </a>
                    <a href="https://scholar.google.com/citations?user=zytlNPAAAAAJ&hl=en">
                        <i class="fab fa-google" style='font-size:20px'></i> |
                    </a>
                    <a href="https://www.linkedin.com/in/yijie-zhang-159852140/">
                        <span class="icon">
                            <i class="fab fa-linkedin" style='font-size:20px'></i>
                        </span> 
                    </a>
                    <br>
                    <a href="docs/CV_GuangyuanZhao.pdf">Download CV </a>
                </div>
<!--                 <div>
                    <a href="https://openreview.net/profile?id=~Guangyuan_Zhao1">Open Review |</a>
                    <a href="https://docs.google.com/document/d/1Lq-C1JCXexD0Ri2KYhScYZwMABS8ci3DX2Dsmx0Ekxg/edit?usp=sharing">My Peer Review Policy |</a>
                    <a href="https://docs.google.com/document/d/1rMCKbsjxoMI_QBwuE5PW5_Q48maHmM3fgQDyoE7gqC4/edit?usp=sharing">Collaborating with Me |</a>
                    <a href="blogs/Blogs_Guangyuan_Write.md">Blogs I Write</a>
                </div> -->
            </div>
        </div>
    </div>
</div>


<div class="container">
    <div class="row vspace-top">
        <div class="col offset-sm-1">
            <h3>Bio</h3>
            <p>
                I am Yijie Zhang, a Ph.D. candidate at UCLA with an anticipated completion in October 2025. Prior to my Ph.D., I earned an M.S. from UCLA in May 2020 and a B.Eng. from Zhejiang University in June 2018. I am actively seeking roles that offer dynamic challenges in the practical application of advanced machine learning and deep learning technologies.
            </p>
            
            <div class="highlight-bg">
                <strong>Research Interests:</strong> My research focuses on the application of generative models in dual-task scenarios, such as super-resolution and cross-imaging modality translation. I am also passionate about machine learning, computational imaging, and computational microscopy. Additionally, I explore machine learning applications in numerical computing, including fluid flow optimization and control.
            </div>
        </div>
    </div>
</div>

<div class="container">
    <div class="row vspace-top">
        <div class="col offset-sm-1">
            <h3>News</h3>

            <div class="row vspace-top-news">
                <div class="col-sm-2 news-date">
                    Oct. 2024
                </div>
                <div class="col">
                    Our paper, "Super-resolved Virtual Staining of Label-free Tissue Using Diffusion Models," is now available on arXiv and under review. <a href="https://arxiv.org/pdf/2410.20073">[paper]</a>
                </div>
            </div>

            <div class="row vspace-top-news">
                <div class="col-sm-2 news-date">
                    Sep. 2024
                </div>
                <div class="col">
                    Our paper, "Virtual Birefringence Imaging and Histological Staining of Amyloid Deposits in Label-free Tissue Using Autofluorescence Microscopy and Deep Learning," has been published in <em>Nature Communications</em>. <a href="https://www.nature.com/articles/s41467-024-52263-z">[paper]</a>
                </div>
            </div>
            
            <div class="row vspace-top-news">
                <div class="col-sm-2 news-date">
                    Feb. 2024
                </div>
                <div class="col">
                    Our paper "Deep Learning-enabled Virtual Histological Staining of Biological Samples" received the 
                    <a href="imgs/LSA_award.jpg" target="_blank">2023 top downloaded paper award</a> at <em>Light: Science & Applications</em>.
                </div>
            </div>

            <div class="row vspace-top-news">
                <div class="col-sm-2 news-date">
                    Oct. 2023
                </div>
                <div class="col">
                    Our review paper "Artificial intelligence-enabled quantitative phase imaging methods for life sciences" is published on <em>Nature Methods</em>.
                </div>
            </div>

        </div>
    </div>
</div>

<div class="container">
    <div class="row vspace-top">
        <div class="col offset-sm-1">
            <h3>Selected Publications</h3>

            <div class='row vspace-top'>
                    <div class="col-sm-4">
                        <img src='imgs/real2sim.png' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Super-resolved virtual staining of label-free tissue using diffusion models
                        </div>
                        <div class='paper-desc'>
                            arXiv
                        </div>
                        <div class='paper-authors'>
                            <u>Yijie Zhang</u>*, Luzhe Huang*, Nir Pillar, Yuzhu Li, Hanlong Chen, Aydogan Ozcan
                        </div>
                        <div>
                            <a href="https://arxiv.org/abs/2404.15259">[Paper]</a>
                             <!--<a href="https://github.com/dcharatan/flowmap">[Code]</a>  -->
                        </div>
                    </div>
                </div>
            
            <div class='row vspace-top'>
                    <div class="col-sm-4">
                        <img src='imgs/Amyloid.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Virtual birefringence imaging and histological staining of amyloid deposits in label-free tissue using autofluorescence microscopy and deep learning
                        </div>
                        <div class='paper-desc'>
                            Nature Communication 2024
                        </div>
                        <div class='paper-authors'>
                            Xilin Yang, Bijie Bai, <u>Yijie Zhang</u>, et al.
                        </div>
                        <div>
                            <a href="https://www.nature.com/articles/s41467-024-52263-z">[Paper]</a>
                             <!--<a href="https://github.com/dcharatan/flowmap">[Code]</a>  -->
                        </div>
                    </div>
            </div>

            <div class='row vspace-top'>
                    <div class="col-sm-4">
                        <img src='imgs/PFU.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Rapid and stain-free quantification of viral plaque via lens-free holography and deep learning
                        </div>
                        <div class='paper-desc'>
                            Nature Biomedical Engineering 2023
                        </div>
                        <div class='paper-authors'>
                            Tairan Liu, Yuzhu Li, Hatice Ceylan Koydemir, <u>Yijie Zhang</u>, et al.
                        </div>
                        <div>
                            <a href="https://www.nature.com/articles/s41551-023-01057-7">[Paper]</a>
                             <!--<a href="https://github.com/dcharatan/flowmap">[Code]</a>  -->
                        </div>
                    </div>
            </div>

            <div class='row vspace-top'>
                    <div class="col-sm-4">
                        <img src='imgs/cair.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Virtual impactor-based label-free pollen detection using holography and deep learning
                        </div>
                        <div class='paper-desc'>
                            ACS sensors 2022
                        </div>
                        <div class='paper-authors'>
                            Yi Luo*, <u>Yijie Zhang</u>*, Tairan Liu, Alan Yu, Yichen Wu, Aydogan Ozcan
                        </div>
                        <div>
                            <a href="https://pubs.acs.org/doi/abs/10.1021/acssensors.2c01890">[Paper]</a>
                             <!--<a href="https://github.com/dcharatan/flowmap">[Code]</a>  -->
                        </div>
                    </div>
            </div>

            <div class='row vspace-top'>
                    <div class="col-sm-4">
                        <img src='imgs/defocus_vs.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Virtual staining of defocused autofluorescence images of unlabeled tissue using deep neural networks
                        </div>
                        <div class='paper-desc'>
                            Intelligent Computing 2022
                        </div>
                        <div class='paper-authors'>
                            <u>Yijie Zhang</u>, Luzhe Huang, Tairan Liu, et al.
                        </div>
                        <div>
                            <a href="https://spj.science.org/doi/full/10.34133/2022/9818965">[Paper]</a>
                             <!--<a href="https://github.com/dcharatan/flowmap">[Code]</a>  -->
                        </div>
                    </div>
            </div>

            <div class='row vspace-top'>
                    <div class="col-sm-4">
                        <img src='imgs/stain_trans.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Deep learning-based transformation of H&E stained tissues into special stains
                        </div>
                        <div class='paper-desc'>
                            Nature communications 2021
                        </div>
                        <div class='paper-authors'>
                            Kevin de Haan, <u>Yijie Zhang</u>, Jonathan E Zuckerman, et al.
                        </div>
                        <div>
                            <a href="https://www.nature.com/articles/s41467-021-25221-2">[Paper]</a>
                             <!--<a href="https://github.com/dcharatan/flowmap">[Code]</a>  -->
                        </div>
                    </div>
            </div>

            <div class='row vspace-top'>
                    <div class="col-sm-4">
                        <img src='imgs/OCT.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Neural network-based image reconstruction in swept-source optical coherence tomography using undersampled spectral data
                        </div>
                        <div class='paper-desc'>
                            Light: Science & Applications 2021
                        </div>
                        <div class='paper-authors'>
                            <u>Yijie Zhang</u>, Tairan Liu, Manmohan Singh, Ege Çetintaş, Yilin Luo, Yair Rivenson, Kirill V Larin, Aydogan Ozcan
                        </div>
                        <div>
                            <a href="https://www.nature.com/articles/s41377-021-00594-7">[Paper]</a>
                             <!--<a href="https://github.com/dcharatan/flowmap">[Code]</a>  -->
                        </div>
                    </div>
            </div>

            <div class='row vspace-top'>
                    <div class="col-sm-4">
                        <img src='imgs/LSA_VS.jpg' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            Digital synthesis of histological stains using micro-structured and multiplexed virtual staining of label-free tissue
                        </div>
                        <div class='paper-desc'>
                            Light: Science & Applications 2020
                        </div>
                        <div class='paper-authors'>
                            <u>Yijie Zhang</u>,  Kevin de Haan, Yair Rivenson, Jingxi Li, Apostolos Delis, Aydogan Ozcan
                        </div>
                        <div>
                            <a href="https://www.nature.com/articles/s41377-020-0315-y">[Paper]</a>
                             <!--<a href="https://github.com/dcharatan/flowmap">[Code]</a>  -->
                        </div>
                    </div>
            </div>

            
            
            <div class="row vspace-top">
                <div class="col-sm-4">
                    <img src='imgs/mfo_teaser.png' class='img-fluid'>
                </div>
                <div class="col">
                    <div class='paper-title'>Model-free Computational Optics</div>
                    <ul>
                        <li style="font-size: 14px;">
                            <u>Guangyuan Zhao</u><sup>†,✉</sup>, Xin Shu<sup>†</sup>, Renjie Zhou<br />
                            <span style="font-size: 14px; font-weight: bold;">High-performance real-world optical computing trained by in situ model-free optimization</span><br />
                            <div style="font-size: 14px; color: #bb1f1f;">
                                <b><u>⭑ Best Paper Award of ICCP'24</u></b>
                            </div>
                            <div class='paper-desc' style="font-size: inherit;">
                                ICCP & TPAMI 2024 [<a href='https://shuxin626.github.io/mfo_optical_computing/index.html'>Project page</a>]
                                [<a href="https://arxiv.org/abs/2307.11957" target="_blank" rel="noopener">arXiv</a>]
                                [<a href="https://github.com/shuxin626/Model-free-Computational-Optics">Code</a>]
                                [Poster]
                            </div>
                        </li>
                        <li style="font-size: 14px;">
                            Guangyuan Zhao<sup>✉</sup>, Renjie Zhou<br />
                            <span style="font-size: 14px; font-weight: bold;">Model-free computer-generated holography</span><br />
                            <div class='paper-desc' style="font-size: inherit;">
                                TENCON 2022 [<a href="https://ieeexplore.ieee.org/document/9978096" target="_blank" rel="noopener">Paper</a>]
                            </div>
                        </li>
                    </ul>
                </div>
            </div>

            <div class='row vspace-top'>
                    <div class="col-sm-4">
<!--                         <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                            <source src="imgs/flowmap_teaser.mp4" type="video/mp4">
                        </video> -->
                        <img src='imgs/real2sim.png' class='img-fluid'>
                    </div>

                    <div class="col">
                        <div class='paper-title'>
                            FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent
                        </div>
                        <div class='paper-desc'>
                            arXiv
                        </div>
                        <div class='paper-authors'>
                            Cameron Smith*, David Charatan*, Ayush Tewari, Vincent Sitzmann
                        </div>
                        <div>
                            <a href="https://cameronosmith.github.io/flowmap/">[Project page]</a>
                            <a href="https://github.com/dcharatan/flowmap">[Code]</a>
                            <a href="https://arxiv.org/abs/2404.15259">[Paper]</a>
                            <a href="https://twitter.com/vincesitzmann/status/1783166394634575931">[Twitter Thread]</a>
                        </div>
                    </div>
                </div>

            <!-- Research Entry: Neural Lithography -->
            <div class="row vspace-top">
                <div class="col-sm-4">
                    <img src='imgs/real2sim.png' class='img-fluid'>
                </div>
                <div class="col">
                    <div class='paper-title'>Neural Lithography</div>
                    <ul>
                        <li style="font-size: 14px;">
                            Cheng Zheng<sup>†,✉</sup>, <u>Guangyuan Zhao</u><sup>†,✉</sup>, Peter T. C. So<br />
                            <div class='paper-title' style="font-size: inherit;">
                                Neural Lithography: Close the Design to Manufacturing Gap in Computational Optics with a 'Real2Sim' Learned Photolithography Simulator
                            </div>
                            <div class='paper-desc' style="font-size: inherit;">
                                SIGGRAPH Asia 2023 [<a href="https://arxiv.org/abs/2309.17343">arXiv</a>]
                                [<a href="https://neural-litho.github.io/">Project page</a>]
                                [<a href="https://github.com/Neural-Litho/Neural_Lithography">Code</a>]
                                [<a href="https://news.mit.edu/2023/closing-design-manufacturing-gap-optical-devices-1213">MIT News</a>]
                            </div>
                            <div>
                                <font size="2"><strong>TL; DR:</strong> &#9312 A real2sim pipeline to construct a high-fidelity neural photolithography simulator + &#9313 a design-fabrication co-optimization framework bridging the design-to-manufacturing gap in computational optics.
                                </font>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>

            <!-- Research Entry: Diffusion World Models for Dextrous Manipulation -->
            <div class="row vspace-top">
                <div class="col-sm-4">
                    <img src='imgs/robot_manipulation.gif' class='img-fluid'>
                </div>
                <div class="col">
                    <div class='paper-title'>Diffusion World Models for Dextrous Manipulation</div>
                    <ul>
                        <li style="font-size: 14px;">
                            <u>Guangyuan Zhao</u>, Nitish Srivastava, Walter Talbott, Shuangfei Zhai, Miguel Angel Bautista Martin, Josh Susskind<br />
                            <div class='paper-desc' style="font-size: inherit;">
                                Apple AI/ML Intern 2020
                            </div>
                            <div>
                                <font size="2"><b>TL;DR:</b> Offline learning of trajectory planning for dextrous manipulation in state space using the diffusion model.</font>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>

            <!-- Research Entry: Blending Diverse Physical Priors with Neural Networks -->
            <div class="row vspace-top">
                <div class="col-sm-4">
                    <img src='imgs/PhysicsNAS.png' class='img-fluid'>
                </div>
                <div class="col">
                    <div class='paper-title'>Blending Diverse Physical Priors with Neural Networks</div>
                    <ul>
                        <li style="font-size: 14px;">
                            Yunhao Ba<sup>†</sup>, <u>Guangyuan Zhao</u><sup>†</sup>, Achuta Kadambi<br />
                            <div class='paper-desc' style="font-size: inherit;">
                                arXiv 2019 [<a href="https://visual.ee.ucla.edu/blendingphysics.htm">Project page</a>]
                                [<a href="https://arxiv.org/abs/1910.00201">Preprint</a>]
                                [<a href="https://github.com/PhysicsNAS/PhysicsNAS">Code with datasets</a>]
                            </div>
                            <div>
                                <font size="2"><b>TL;DR:</b> An all-in-one method excelling in blending physical priors with neural networks across diverse physical and data conditions.</font>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>

            <div class='vspace-top'>
                <h4>Previous Work in Computational Microscopy:</h4>
                [Summary of what I did in my master stage]
            </div>

            <!-- Research Entry: Nonlinear Focal Modulation Microscopy -->
            <div class="row vspace-top">
                <div class="col-sm-4">
                    <img src='imgs/NFOMM.png' class='img-fluid'>
                </div>
                <div class="col">
                    <div class='paper-title'>Nonlinear Focal Modulation Microscopy</div>
                    <ul>
                        <li style="font-size: 14px;">
                            <u>Guangyuan Zhao</u><sup>†</sup>, Cheng Zheng<sup>†</sup>, Cuifang Kuang, et al.<br />
                            <div class='paper-desc' style="font-size: inherit;">
                                Physical Review Letters 2018 (On the cover) [<a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.193901">Paper</a>]
                            </div>
                            <div>
                                <font size="2"><b>TL;DR:</b> Combining PSF engineering with non-linear light-matter interaction achieves STED-level resolution with a simpler setup.</font>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>

            <!-- Research Entry: 3D Super-Resolved Multi-Angle TIRF via Polarization Modulation -->
            <div class="row vspace-top">
                <div class="col-sm-4">
                    <img src='imgs/P_TIRF.png' class='img-fluid'>
                </div>
                <div class="col">
                    <div class='paper-title'>3D Super-Resolved Multi-Angle TIRF via Polarization Modulation</div>
                    <ul>
                        <li style="font-size: 14px;">
                            Cheng Zheng, <u>Guangyuan Zhao</u>, et al.<br />
                            <div class='paper-desc' style="font-size: inherit;">
                                Optics Letters 2018 [<a href="http://zhaoguangyuan123.github.io/Pol-TIRF/">Project page</a>]
                                [<a href="https://www.osapublishing.org/ol/abstract.cfm?uri=ol-43-7-1423">Paper</a>]
                                [<a href="https://github.com/zcshinee/Pol-TIRF">Code</a>]
                                [<a href="https://drive.google.com/file/d/1nchpdxzIxDw5OX-rdb7TsBwnD-RBWYXx/view?usp=sharing">Slides</a>]
                            </div>
                            <div>
                                <font size="2"><b>TL;DR:</b> Polarization serves as a lightweight add-on for boosting resolution in super-resolution imaging.</font>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>

            <!-- Research Entry: Resolution Enhanced SOFI via Structured Illumination -->
            <div class="row vspace-top">
                <div class="col-sm-4">
                    <img src='imgs/SI-SOFI.png' class='img-fluid'>
                </div>
                <div class="col">
                    <div class='paper-title'>Resolution Enhanced SOFI via Structured Illumination</div>
                    <ul>
                        <li style="font-size: 14px;">
                            <u>Guangyuan Zhao</u>, Cheng Zheng, Xu Liu, Cuifang Kuang<br />
                            <div class='paper-desc' style="font-size: inherit;">
                                Optics Letters 2017 [<a href="http://zhaoguangyuan123.github.io/Si-SOFI/">Project page</a>]
                                [<a href="https://www.osapublishing.org/ol/abstract.cfm?uri=ol-42-19-3956#Abstract">Paper</a>]
                                [<a href="https://github.com/zhaoguangyuan123/SI-SOFI">Code</a>]
                                [<a href="https://docs.google.com/document/d/1XN3kaMzeuoVbQQZ_3gzaOTxqVapGV7ZBeWnIu_Z3pZc/edit">Reviewers' Comments</a>]
                            </div>
                            <div>
                                <font size="2"><b>TL;DR:</b> Combining structured illumination and stochastic fluorescence emission further enhances super-resolution.</font>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</div>

<div id='footer' class='vspace-top'></div>



</body>
</html>

    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) 
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed 
    <script src="js/bootstrap.min.js"></script>




                <div class="expandable-section">
                <button class="toggle-button">Review Policy: I am happy to be the reviewer for journals and conferences, with following preferences</button>
                
                
                <div class="content">
                    <div>
                        <center>
                            <!-- <figure style="width: 100%;">
                                <a>
                                    <img width="80%" src="asserts/HOE_results.png">
                                </a>
                                <p class="caption" style="margin-bottom: 24px;"><br>
                                    We show improvement in performance when design the holographic optical elements(HOE)
                                    w/ our learned litho model.
                                </p>
                            </figure> -->
                          <!-- - knowledge advacement over quatity of the work.
                          - the work is well-organized and well-presented.
                          - the work is well-motivated and well-justified.
                          - the work is well-validated and well-evaluated.
                          - the work is well-communicated and well-discussed.

                          
                          I want to see the paper that have surprise. I view novelty first. Yes I will punish offers low score to the paper that is too obvious to me.
                        </center>
                    </div>
                    <div>

                        <center>
                            <figure style="width: 100%;">
                                <a>
                                    <img width="80%" src="asserts/D2_HOE_quantative.png">
                                </a>
                                <p class="caption" style="margin-bottom: 24px;"><br>
                                    We <b>quantitatively</b> show improvement in performance when design the holographic
                                    optical elements(HOE) w/ our learned litho model.
                                </p>
                            </figure>
                        </center>
                    </div>

                </div>
            </div> -->



